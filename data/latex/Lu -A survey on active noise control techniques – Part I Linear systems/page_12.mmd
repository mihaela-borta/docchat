A computationally fast algorithm that uses the _binormalized data reusing_ was proposed in [181], based on the FxLMS algorithm. Related to the binormalized data reusing is the _data reusing_ method [182]. The filtered-x data reusing algorithm solves the same optimization problem associated with the classical AP approaches, but employs an iterative strategy to define the projection onto a set of hyperplanes instead of using past information directly.

Based on the fractional lower order moment (FLOM) criterion, a class of the filtered-x least mean \(p\)th power (FxLMP) algorithms was investigated for robust performance in the presence of \(\alpha\)-stable noise [182, 183, 184]. A representative algorithm of this type is the modified normalized FxLMP (MNFxLMP) algorithm [184]. Furthermore, a filtered-x general step size NLMS (FxgsnLMS) algorithm [178] and a companding FxsgnLMS algorithm [185] were developed. The essence of the former is the adaptation of Gaussian kernel, and the latter is based on the instantaneous power of the companded error signal. In [186], an online estimation approach for non-Gaussian noise characteristics was developed and then was incorporated into the sign FxLMS and FxLMP algorithms. Such algorithm avoids the selection problem of threshold parameters \(c_{1}\) and \(c_{2}\), and it is easy to implement.

The Akhtar's algorithm and its variants utilized the hard limit to clip the residual noise and the reference signal [187, 188]. Accordingly, the algorithm in [189] originally introduced a soft bound for residual noise and reference signal, which offers lower noise reduction level for impulsive noise. Recalling the M-estimation has robustness for system identification, several variants of the M-estimate algorithm were also presented for AINC [190, 191, 192, 193].

Let us define \(J(n)\) as the cost function and \(\Phi(e)=\partial J(e)/\partial e\) as the score function. Fig. 5 shows the score function \(\Phi(e)\) in the FxLMS-based algorithms. As can be seen, the score function of the FxLMS algorithm is unbounded. In contrast, the M-estimator can bound the outliers from impulsive noise. The algorithm in [193] used a family of robust estimators, such as _Huber_, _Fair_, and _Hample_ for combating impulsive noise, and further utilized the threshold scheme from Akhtar's algorithm. For a fair comparison, the averaged noise reduction (ANR) is usually employed as a performance measure, which is defined by [177]

\[\text{ANR}(n)\triangleq 20\log\left\{\frac{A_{e}(n)}{A_{d}(n)}\right\} \tag{6}\]

where \(A_{e}(n)=\chi A_{e}(n-1)+(1-\chi)|e(n)|\) and \(A_{d}(n)=\chi A_{d}(n-1)+(1-\chi)|d(n)|\), and \(\chi=0.999\). In Fig. 6, we compare the ANRs of the representative algorithms. The primary path \(P(z)\) and the