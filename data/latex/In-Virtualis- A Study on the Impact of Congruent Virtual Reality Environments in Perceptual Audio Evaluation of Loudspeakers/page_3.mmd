s/he hears. In terms of the realisation of sound fields in the laboratory, studies have shown that a divergence between the listening room and the synthesised sound field decreases the perceived externalisation [3], and distorts the ability of listeners to correctly judge the perceived distance of sound sources[4].

Moreover, even low level auditory processing tasks such as sound localisation, have been shown to rely on stored stimulus patterns [5]. It has been therefore argued that spatial perception is a top-down process [6], which involves prior knowledge and expectations about the experience of the perceived stimulus, in a multimodal manner. Recently, Udesen et al. [7] indicated that a system that fails to match the clear expectations of realistic environments that human possess', e.g., on the amount of reverberation of a small room, would degrade the realism of the aural experience significantly.

In order to control alterations of assessor's judgements from non-aural inputs, it is common that auditory stimuli are evaluated either in the same environment, and/or under dark conditions. Although, practically this interaction forms a statistical constant across stimuli sets, the multimodal integration of the information is missed and therefore, the ecological validity of the test decreases [8, 9]. This would be especially critical when complex acoustical fields are to be evaluated.

Following the literature, this paper investigates the extent to which the presentation of congruent immersive virtual environments, i.e. Virtual Reality (VR), could facilitate a more natural experimental setting to the human assessors whilst operating on controlled experimental design in a laboratory [10, 11]. It is hypothesized that a congruent audio-visual experience would improve the perceived _plausibility1_[12] of the auralised field. In consequence, a higher degree of plausibility would facilitate a more natural and efficient audio evaluation task to human assessors. These improvements could therefore improve the perceived _Quality of Experience_ (QoE) [13]. The main research questions were:

Footnote 1: In [12] plausibility is defined as “a simulation in agreement with the listener’s expectation towards an equivalent real acoustic event”

[RO1]Do Basic Audio Quality (BAQ) ratings differ when displaying VR visuals of the auralised room?

[RO2]Is the sense of plausibility increased when displaying VR visuals of the auralised room?

[RO3]Is difficulty reduced, and efficiency affected, when displaying VR visuals of the auralised room?

[RO4]Is the QoE increased when displaying VR visuals of the auralised room?

[RO5]Is a VR Listening Test interface based on the current real-world interaction a usable system?

In section 2 the method is presented, followed by the results in section 3 that are further discussed in section 4. Finally concluding remarks are highlighted in section 5.

## 2 Method

The experimental method followed a 2 (Visual Conditions)\(\times\)3 (Sources)\(\times\) 3 (Programmings) within subject design. In detail, human assessors evaluated a set of sound fields captured in a room, when excited by three sources, using three programme materials. The resulting fields were evaluated under two visual conditions.

Two experimental tasks were followed for each visual condition. First, a listening test was completed, assessing the Basic Audio Quality (BAQ) of the three sources, followed by an assessment of the overall experience. A usability questionnaire was also used, to track the assessors' response to the experience of the VR environment and interface.

For designing the listening test, ITU method recommendations from [14] were followed with some modifications. This nine stimuli (3 sources \(\times\) 3 programmes) were grouped in three different pages, corresponding each page to a different programme. In addition, for every page, one of the sources was randomly repeated in order to test participants reliability, i.e. considerably different ratings for the same source imply a non reliable participant. The presentation order was always randomised for both sources and programmes, in order to avoid any possible bias and, for all pages, the definition of BAQ was displayed to make sure participants were completely certain of what to evaluate.

The ratings were required to be given according to the Continuous Quality Scale (CQS) [14], which has a range of integer values from 0 to 100 divided into five equal intervals, corresponding each interval to an adjective that describes the degree of quality perceived: _Bad, Poor, Fair, Good, Excellent_, obtaining thus qualitative