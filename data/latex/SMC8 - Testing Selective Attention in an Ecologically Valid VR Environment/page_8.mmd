which may result in them not interacting with virtual characters (i.e., the waiter) as if they were real human beings (e.g., see [10]). Therefore, it is difficult to determine whether the eye-tracking data obtained from the experiment is comparable to a real-world scenario. Nonetheless, there are indications that the virtual environment designed in the study detects challenges in selective attention for individuals with hearing loss. The group with the hearing loss simulator running spent more time focusing on the waiter while he spoke, which might imply that they had to concentrate more on him than the group with normal hearing to comprehend what he was saying (see Figure 7). Even though the participants with hearing loss spent more time looking at the waiter, they understood less of what he was saying than the group with normal hearing (see Table 6). The high standard deviation for the eye-tracking results of both groups makes it difficult to conclude whether the average represents the participant groups. However, the results were similar when comparing the HL results to the participant with an actual hearing impairment (see Table 7). This suggests that eye-tracking data can potentially be useful in identifying individuals with hearing impairments in certain cases.

One factor that could introduce bias in the results is that even though lip-syncing was used for the characters in the scene, the mouth movements generated by Oculus Lip Sync might not have been realistic enough for people to be able to use lipreading. Since lipreading is an important tool for speech recognition for hearing-impaired individuals [26], it could potentially impact the comprehension level of these individuals during their participation in the test. Another confounding variable that might have affected the data could be a lack of precision in the measurements. We did not test the exact error of the eye tracking measurements, but the algorithm could not accurately register when someone quickly glanced at an object. This would need to be tested properly in further iterations, and screen recordings of the user's experience should be made to compare to the data. Furthermore, it should be considered whether eye-tracking data can be reliable when testing for auditory attention or whether the found connection is due to other confounding variables. The test included two auditory events (i.e., glass breaking and loud laughter). The eye tracking did not reveal any difference between groups in whether they looked towards the auditory events, and whether the eye tracking captured the eye movement correctly is uncertain. 5 out of 12 participants did look at the point where the glass breaking sound originated. This indicates that the directional cues from the specialization were precise enough for the individuals to guess the direction.

The IPQ scores suggest that the prototype delivered a decent level of realism and presence, and the scores do not seem to differ between the two groups tested. Yet, there is still room for improvement. To attain ecological validity, prototypes must closely replicate real-life situations. Therefore, future iterations should enhance the scenario's different aspects, such as graphics, animations and recorded conversations.

When the participants were asked whether they felt that the sound was similar to being in a real cafe, the average score was above average at 3.31 (see Table 4). Furthermore, when people were asked from which directions they could distinguish speech, the results correlated well with the actual direction of the speech sound sources in the scene. A noticeable difference was found when comparing NH, and HL results from directions (see Figure 8 and 9), indicating that the hearing loss simulator caused the participant to perceive less sound from the left and right conversation. Steam Audio (and its generic HRTF) seems to have generally been a viable option for spatialization and room acoustics. To which degree an individualized HRTF would have improved these results is unknown. The high-frequency hearing loss simulation may cause the directional cues from the generic HRTF to be less precise. Furthermore, true-to-world test results require the audio to be as close to real life as possible, leaving space for improvement. It could be helpful to assess whether using recorded RIRs, like in [8], would increase the perception of direction in the scene. Even though it would limit the possible source-listener configurations and restricts movement in the scene, it could be a required trade-off.

## 8 Conclusion

A prototype for an ecologically valid VR experience to assess individuals' abilities in selective attention has been successfully developed. A natural everyday setting, where selective attention often becomes challenging, was implemented and evaluated. Generally, the data from the eye-tracking seems to detect differences between the two participant groups. The high-frequency hearing loss simulator seems to have changed the participants' behavior, leading to participants relying more on vision to compensate for lack of hearing. The same tendency was found when comparing these results to the results from the participant with hearing impairment. However, given that only 13 participants have been tested, it is difficult to determine whether these observed differences are a general tendency. Therefore, it is essential to investigate the effect of a larger sample size in future iterations. Furthermore, future iterations should include a participant group of actual hearing-impaired individuals to exclude relying on a simulator. Additionally, the realism achieved for audio and visuals leaves room for improvement and may also have had an influence. Nevertheless, the results from the evaluation indicate that the prototype achieved a high enough degree of ecological validity while measuring differences in the two participant groups' behavior. This implies that the prototype can test for a deficiency in auditory selective attention in hearing-impaired individuals.

#### Acknowledgments

We would like to thank our supervisor Stefania Serafin for her guidance throughout the project. We would also like to thank Nina Karpinska and Tommy Rushton for their voice acting.