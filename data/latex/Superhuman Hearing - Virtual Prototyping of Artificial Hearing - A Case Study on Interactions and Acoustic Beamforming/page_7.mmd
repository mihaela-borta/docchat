We conducted a first test where the users were required to complete a small training session (Sec. 4.2.1) and calibration of the directivity parameters for all virtual sound sources (Sec. 4.2.2), followed by a speech pairing task in an abstract VE (Sec. 4.2.3). At this first stage, the goal was to offer the participants a minimal visual environment where the main cues were restrictively auditory by design. The second test was built so that participants performed the task within a 360\({}^{\circ}\) video recording where they were surrounded by real speakers in order to recreate a realistic and multimodal (audio-visual) scenario. The main research questions related to this user study are:

* RQ1: What are the main preferences for directivity parameters?
* RQ2: Can users modulate their performance in the presence of realistic visual references?
* RQ3: Will user performance differ between interaction techniques?

We collected data from 17 participants (age 28 \(\pm\) 6 years) with self-reported normal hearing. All participants were informed about the experiment and gave consent to data collection.

### Stimuli

The first immersive VE consisted of eight blue virtual cubes defining eight abstract sound sources in a minimalistic open space with light blue sky and gray floor (see Fig. 0(a)). The cube directly in front of the listener (i.e. the focus source) changed color dynamically based on the user's spatial position when rotating in the scene. Virtual sources were distributed in a perfect circle, separated by a 45\({}^{\circ}\) angular distance.

The sound sources were simultaneously played in loops and comprised sets of sentences available from IEEE Recommended Practice for Speech Quality Measurements [1], also known as the Harvard Sentences, used in many different fields of audio engineering (e.g. speech-to-text software and cochlear implants testing). These sentences are considered standard and an optimal research material because all the word lists are phonetically balanced, meaning that the frequency of sounds in these lists corresponds to that of natural language and conversations. The sentences are short and simple; that is, monosyllabic words punctuated by exactly a single two syllable word sentence.

For the 360\({}^{\circ}\) video recording setup, 8 concurrent human speakers were placed around the listener's position in an anechoic room every 45\({}^{\circ}\) starting from directly in front (see Fig. 0(c) for a view map of the recordings imported in unity). The recording contained a 360\({}^{\circ}\) video, captured with _Garnim Vripb 360\({}^{\circ}\)_ camera7, of the speakers reading randomly selected Harvard Sentences, and the audio signal from each speaker was recorded with clip microphones, DPA 5C40608 (see Fig. 0(c) for two pictures from the recording sessions). The signal cross-feed between microphones was \(\approx\)15 dB and it was later reduced with equalization and compression to \(\approx\)10 dB difference between the main signal and interfering signal. After the post-cofiting both videos and audio signals, the auditory speech stimuli were calibrated at 60 dB SPL, equivalent to a conversational signal level.

Footnote 7: [https://bury.garnim.com/data-DK/DK/p562010](https://bury.garnim.com/data-DK/DK/p562010)

Footnote 8: [https://www.dynamicphones.com/dscreet/4060-series-miniature-omidrictional-microphone](https://www.dynamicphones.com/dscreet/4060-series-miniature-omidrictional-microphone)

For both abstract and realistic scenarios, there were four pairs of two voices: a female and a male speaker, respectively. For each pair, seven sentences (sets hereafter) were selected from Harvard Sentences and synchronously assigned to both sources/speakers. Such a total of 28 sentences were the basis for the creation of seven balanced configurations assigned to participants in our study: sentences order for each set, female-male pairing, and the combination of a set with a pair during the voice synthesis/recording were made following latin squares.

### Protocols

Within-subjects task-based tests were conducted in order to identify meaningful interpretations within the two VEs. Participants were asked to find pairs of the same sequences in a group of eight speakers. They were asked to change the shape and the width of a directivity beam for several directions optimizing each interaction technique introduced in Sec.3.3. The three different types of interaction were individually tested by each participant in a within-subject experiment. Moreover, a normal listening condition, (i.e., no hearing aids tool provided) were included to test the level of difficulty of the proposed task. The experiment procedure consisted of two different test blocks with multiple days between the two tests in order to reduce learning effects (see Fig. 5(a) and 5(b) for protocol schematics). The first session accounted for training, calibration and pairing task with abstract visuals. The second part evaluated the task difficulty and the realistic VE.

#### 4.2.1 Training

Participants were asked to do a brief tutorial where they became familiar with the HTC View platform and the audible effects of changing both directivity and gain values with three different sources in the space. This training procedure helped reduce biases introduced by different confidence levels with VR technologies. We defined four levels:

* (1) an introduction to the hand controller, where the participants were asked to increase the volume for the focus source and shape the directivity of another cube into a very narrow bipolar pattern;
* (4) the virtual beamformer was already calibrated with arbitrary values for both gain and directivity so that the subject would only need to try to move around using the specific interaction control to select the focus source among the increasing number of concurrent sources.

This training procedure could be repeated as often as necessary.

Fig. 5: Recording sessions and positions of the eight speakers.

Fig. 6: User study, part 1 and 2: protocol schematics.