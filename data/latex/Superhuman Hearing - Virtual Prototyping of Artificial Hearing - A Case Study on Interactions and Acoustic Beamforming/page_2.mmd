# Superhuman Hearing - Virtual Prototyping of Artificial Hearing:

a Case Study on Interactions and Acoustic Beamforming

Michele Geronazzo, Luis S. Vieira,

Niels Christian Nilsson, Jesper Udesen, and Stefania Serafin

Michele Geronazzo, Niels Christian Nilsson, and Stefania Serafin are with Aalborg University Copenhagen, Department of Architecture, Design, and Media Technology, Copenhagen, Denmark. E-mail: {jnee,no,ns}@create.aau.dk. Luis S. Vieira is with Aalborg Virtual Reality, Copenhagen, Denmark. E-mail: lus@hzhorov.com. Jegper Udesen is with GN Audio A/S, Ballerup, Denmark, E-mail: jddesen@jnova.com Manuscript received xx.xx. 201X; accepted xx.xxx. 201x. Date of Publication xx.xx. 201X; date of current version xx.xx. 2012x. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org. Digital Object Identifier: xx.xxx/TVCG.2012.xx.xxxxxx

###### Abstract

_Directity and gain in microphone array systems for hearing aids or hearable devices allow users to acoustically enhance the information of a source of interest. This source is usually positioned directly in front. This feature is called acoustic beamforming. The current study aimed to improve users' interactions with beamforming via a virtual prototyping approach in immersive virtual environments (VEs). Eighteen participants took part in experimental sessions composed of a calibration procedure and a selective auditory attention voice-pairing task. Eight concurrent speakers were placed in an ensemble environment in two virtual reality (VR) scenarios. The scenarios were a purely virtual scenario and a realistic 360deg audio-visual recording. Participants were asked to find an individual optimal parameterization for three different virtual beamformers: (i) head-guided, (ii) eye gaze-guided, and (iii) a novel interaction technique called dual beamformer, where head-guided is combined with an additional hand-guided beamformer. None of the participants were able to complete the task without a virtual beamformer (i.e., in normal hearing condition) due to the high complexity introduced by the experimental design. However, participants were able to correctly pair all speakers using all three proposed interaction metaphors. Providing superhuman hearing abilities in the form of a dual acoustic beamformer guided by head and hand movements resulted in statistically significant improvements in terms of pairing time, suggesting the task-relevance of interacting with multiple points of interests._

 Virtual prototyping, Sonic interactions, Acoustic beamforming, Artificial hearing, Virtual reality, Multi-speaker scenario

## 1 Introduction

A central feature of human hearing is the listener's ability to focus attention to a certain direction, for example towards a specific sound source [40]. This selective auditory attention relies on a set of mechanisms that work together to produce an understanding of the direction of an incoming sound, as well as what physical features characterize its source, ultimately creating an auditory representation of the source. In this way, the auditory system analyzes the sound field to determine what is relevant content and what is noise. However, listening is a multi-modal experience, where the auditory representation interacts with other modalities as well as with bodily and cognitive mechanisms. It is important to understand to what degree these mechanisms--motion, visual feedback, spatial directivity--influence the listening process during a specific task [62], how they support the auditory behavior, and to what extent they can be used to support artificial hearing devices such as hearing aids and smart headphones, called _hearables_[61]. The ultimate goal of this research is to control these interactions in order to create a set of tools that provide _superhuman hearing_.

People without hearing impairments are usually able to distinguish between meaningful and non-meaningful auditory information; thus, solving the cocktail party problem [11]. On the other hand, this is one of the main challenges for people with hearing impairments who require artificial hearing and hearing aids [61] equipped with digital signal processing algorithms such as _beamforming_[49, 70]. The current work is based on the following assumption and long-term vision: even though the currently available technologies do not allow a perfect separation of relevant signals and background noise, one can realistically assume that future developments will raise the bar to make this feature available. This can for example materialize by taking advantage of artificial intelligence to analyze the sound field and separate individual sound sources in adverse listening situations [76]. In the future, when hearing aids can separate individual sound streams in a cocktail party environment, it will be particularly relevant to know how these sound streams should be processed before they are presented to the hearing aid user. One obvious presentation mode would be to preserve

Fig. 1: Screenshots of the two virtual listening scenarios with map (left) and listener (right) view. Beamforming directivity patterns are also visualized together with an iconic representation of an ideal beamformer (b). Since the abstract scenario (a) was a virtual environment with minimal visual feedback, the realistic scenario (c) was rendered though immersive audio-visual 360Â° recordings.