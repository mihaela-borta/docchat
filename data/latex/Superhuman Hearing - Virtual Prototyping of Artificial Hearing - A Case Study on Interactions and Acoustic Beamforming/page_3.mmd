the natural spatial characteristics of the individual sound streams by filtering them with the listener-specific acoustic information. But other more artificial presentation modes could also benefit the hearing aid user, like having a narrow acoustic beam following the head orientation where only sound sources within the beam are presented to the user. Identifying the optimal presentation mode is therefore relevant from a hearing aid technology perspective.

Accordingly, our main research focus relates to optimal interaction between audio streams, which is based on _a priori_ knowledge of the separated sound sources and the specific task performed by the listener. From a methodological point of view, our experimental sessions focused on novel forms of interaction aiming to reinforce the listener's selective auditory attention in different situations: from nearly impossible tasks without technological support to aiding solutions for hearing impaired users in practical real-life scenarios. Our case study considered an extremely challenging scenario with multiple concurrent speakers where spatial filtering via beamforming control [70] allowed normal hearing users to acoustically amplify a specific speaker while attenuating unwanted sound sources.

Virtual Reality (VR) was chosen as a prototyping platform because it provided flexibility and faster prototyping of sonic interactions [65] as well as control parameters over different simulations. This paper proposes a platform for virtual prototyping of interactions with hearing aids, where multiple artificial hearing models can be tested. We included two different virtual environments (VEs) providing different levels of realism: (i) one involving minimal visual feedback with eight virtual cubes displayed in a perfect circle around the listener (see Fig. 1a), and (ii) another showing eight real human speakers in the same positions recorded in a 360deg video (see Fig. 1c). The participants could individually calibrate a _virtual beamforming_ (for an iconic representation see Fig. 1b) with three degrees of freedom: gain, width, and shape. Moreover, the participants were able to control the beam's direction using three different interaction techniques: (i) a standard head pointing, (ii) a more natural eye-gaze pointing recently proposed in [23, 7], and (iii) a novel sophisticated dual beamforming control based on head and hand orientations. In this case study, we investigated the efficacy of the proposed interactions and parameters in terms of completion time, correctness, and perceived task load when trying to find multiple pairs of the same speech segments in highly chaotic situations where the natural listening condition is insufficient to accomplish the task.

The paper is organized as follows. Sec. 2 surveys the theoretical background of the cognitive listening process, hearing aids technologies, and virtual prototyping of optimal hearing support in VR. In Sec. 3, we focus on beamforming and directivity patterns as a tool for artificial hearing. Moreover, this section describes the finanural spatial audio rendering with higher-order ambasions (HOA) [29] and the technical implementations of the proposed VEs. In Sec. 4, the experimental protocol is described, and Sec. 5 presents the data collected, providing statistical comparisons among interactions and scenarios. Finally, Sec. 6 and 7 discuss the outcomes of the study, concluding with a summary of meaningful contributions and future work.

## 2 Background

Listening has been an important part of human selective attention research since 1953, starting with Cherry's research on the "cocktail party problem" [14], and the use of dichic stimuli to test speech intelligibility. Different levels of perception and cognition contribute to human's ability to segregate signals--also referred to as auditory signal analysis [10, 11]. When confronted with multiple simultaneous stimuli (speech or non-linguistic stimuli), it is necessary to segregate relevant auditory information from concurrent background sounds and to focus attention on the source of interest [11, 14]. This action is related to the principles of auditory scene analysis that require a stream of auditory information filtered and grouped into a number of perceptually distinct and coherent auditory objects. Studies on spoken language processing suggest that in multi-talker situations, auditory object formation and selection together with attentional allocation contribute to define a model of cocktail-party listening [32, 66]. Accordingly, Ahrens et al. [2] recently conducted a pilot study with six participants who were able to accurately analyze virtual audio-visual scenes containing up to six concurrent talkers.

### _Dichotic listening and masking effect_

Dichotic listening is a psychological test used to investigate selective auditory attention and shows the brain's ability of hemispheric lateralization for speech perception--a feature of importance when listening to different acoustic events presented to each ear simultaneously [33, 50]. The right-ear advantage is an interesting finding [39] revealing the direct anatomic connection of the right ear to the left hemisphere, which in most people is specialized in language processing.

Moreover, another relevant aspect of dichotic listening is the effect of interfering speech or other concurrent non-linguistic signals due to their frequency spectrum characteristics and spatial information. Of particular interest here is the anatomy of the human body, head, and ears that introduces a listener-specific acoustic characterization of the stimuli through the so-called head-related transfer function (HRTF), helping the brain to localize the sound in space [74]. The interaural time differences (ITD) and interaural level differences (ILD) allow listeners to locate sound sources on the horizontal plane, and they play an important role in generating high levels of speech recognition in complex listening environments [14, 45]. ITD and ILD combined improve robustness from the masking effect--when the signal of interest shares information in the same frequency bands and/or the same sound pressure level with interfering signals--and increase the decorrelation between left and right ear and thus the separation between background noise and meaningful sounds [75]. The brain also correlates the signals that arrive at both ears through the so-called interaural cross-correlation coefficient (IACC), which is a measure associated with the feeling of spaciousness and envelopment in room acoustics: the higher the value, the more spacious and comfortable the space feels to the listener [8].

### _Hearing impairment_, hearing aids and artificial hearing

Understanding speech in noisy situations becomes a very difficult task for people with hearing impairments when both speech and noise coexist above their hearing threshold. In such individuals, the ability to focus attention only on the important stimuli benefits from an increase of signal-to-noise ratio (SNR) with respect to masking sources for optimal intelligibility. This is particularly pertinent to multi-speaker scenarios (for a recent review see the work of Falk et al. [22]). Typical hearing losses are located in the cochlea where damage to hair cells can be observed. This damage is often provoked by exposure to loud sound [57]. The hearing threshold, also known as speech reception threshold, defines the lowest level at which a person can separate meaningful signals from noise. This value ranges from a few dB to more than 10 dB, causing severe problems of communication. In many cases, the resonance effect and corresponding frequency perception are deteriorated due to the damage of the outer hair cells. Consequently, the brain is no longer able to benefit from the long-term spectrum fluctuations, where speech is recognized to have larger variance [11] and from the spatial cues which are able to reduce masker interference often referred to as spatial release from masking [26, 57].

Hearing aids equipped with microphone arrays in behind-the-ear (BTE) and in-the-ear (ITE) configurations aim at compensating for these hearing impairments. More recently, alternative designs have come to the market. These are smaller in size featuring a thinner sound tube that connects the hearing device behind the ear to the ear canal, called receiver-in-canal (RIC). Signal processing requirements of for hearing aids are very restricted due to the physical size of the device and optimized due to energy consumption. In general, the signal flow starts by capturing the acoustic input with a microphone array, typically composed of three microphones, which is processed into a single signal within the directional microphone unit. The main frequency-band-dependent processing steps are noise reduction and signal amplification combined with dynamic compression. To address the problem of strong masking and to increase the SNR of the signal output, beamforming or other noise reduction approaches are usually developed [26].