#### 4.2.2 Calibration

The high variability in auditory spatial perception [25] and cognition [4] among listeners requires to consider biases due to user auditory profiles that might be difficult to model, negatively affecting the final results. Accordingly, participants performed a calibration in VR for each interaction metaphor in the abstract VE surrounded by all eight virtual cubes. The main goal was to identify optimal values of directivity parameters towards each source in order to enhance the natural intelligibility of the frontal sound source (i.e., the focus source) on individual basis. Accordingly, participants were asked to change the values of gain level, directivity alpha, and sharpness of the virtual beamformer related to the focus source, so that they could clearly understand what was being said by the focus voice. Moreover, the calibration procedure also allowed to select direction-dependent parameters for the remaining sources: participants pointed the hand controller towards each masking source and manipulated a sub-beam according to the mapping of Fig. 4 while maintaining the selection on the focus source with head/eye control.9

Footnote 9: In HC, the hand controller did not work as an interaction tool for beam control but acted as remote controller for an additional H parameter adjustments.

The resulting virtual beamformer could potentially exhibit a multi-lobe radiation pattern. Its parameter values were then used to understand preference distributions, and to practically compensate different user sensitivities during the tests. Since our user population had a limited size, our approach was considered less error prone than considering fixed parameter values that would lead to hardly explainable listener-beamformer mixed effects.

#### 4.2.3 Test

For each beamforming interaction, participants were asked to pair the sources matching the set of sentences. After exposure to both abstract and realistic visuals, the perceived workload was assessed using a modified version of the NASA task load index (TLX) [27] called the NASA Raw TLX (RTLX) [28] in order to identify usability issues on performances due to the beamforming control. The questionnaire included six 5-point rating scales, ranging from 1 to 5 (high ratings indicated high task load). The questionnaire was administered after exposure to each condition and yielded an aggregate score serving as an estimate of the overall perceived workload. Two additional questionnaires were administered after exposure to the realistic scenario. That is, participants' confidence with the three interaction techniques was also assessed by means of the short version of the _user experience questionnaire_ (UEQ-S) [64] and the _questionnaire for the subjective consequences of intuitive use_ (QUESI) [51]. The UEQ-S includes eight of the 26 semantic difference scales of the original UEQ [43], and the mean of the participants responses to the eight items, the _UX score_, is viewed as a measure of the overall user experience. The QUESI includes 14 items, organized into five subscales pertaining to subjective mental workload, perceived achievement of goals, perceived effort of learning, familiarity, and perceived error rate. The mean of the five subscales, the _QUESI score_, is taken as a measure of the how intuitive participants found the interaction. For the sake of consistency all three questionnaires included 5-point rating scales, where high scores indicated high perceived workload (RTLX), an overall positive user experience (UEQ), and high intuitiveness (QUESI).

The presentation order of the three interaction metaphors was randomized. Participants were always informed about the current interaction metaphor, but no further information regarding the positions of the pairs or which pairs were correct was given.

## 5 Data analysis and results

It is very relevant to note that none of the participants were able to find the first pair (and thus the remaining pairs) in the natural listening condition within the VEs before quitting the experimental session for the high level of difficulty. No statistical analysis was based on this information, even if this result certified the impracticability of such pairing task without artificial hearing support.

The analyzed metrics were:

* **correct pairing**: number of correct pairs found during exposure to each interaction metaphor;
* **pairing time**: time spent exploring the scene while finding the next pair of speakers;
* **pairing action time**: the time required to actually select the two speakers identified as a pair during the pairing time.

All data were treated as interval or ratio data. Significant outliers were identified based on the inspection of boxplots, and Shapiro-Wilk's tests were used to determine if the data were normally distributed. If no outliers were detected and the data was assumed to be normally distributed, one-way repeated-measures ANOVAs were used for statistical comparison. Alternatively, non-parametric Friedman tests were used for statistical analysis, and pairwise comparisons performed using Dunn-Bonferroni tests. All cross-study comparisons were performed using non-parametric methods due to violations in the normality assumption. Specifically, for each measure three Mann-Whitney U tests were used to compare the results obtained during exposure to H, HE and HC in the study involving abstract visuals with the corresponding conditions in the study involving realistic stimuli.

In case of four of the 18 participants exposed to the abstract visuals and four of the 17 participants exposed to realistic visuals, logging errors prevented us from obtaining information about the pairing data. Because these errors did not affect the participants' experiences, the analyses of the questionnaire data involved the full samples. However, the same analyses were also run on the reduced samples for abstract visuals (n = 14) and realistic visuals (n=13). Only discrepancies between the two sets were reported along with the analyses of the full sample.

### Calibration:

In Fig. 7 (bottom), histograms show the data distributions of alpha, sharpness and gain values of the calibration procedure. It is worthwhile to notice that most participants kept the starting values for alpha (0) and sharpness (1) changing only the gain values for each considered direction of the virtual beamformer, shaping their own polar pattern. Accordingly, we conducted the statistical analysis for the gain parameter only. Fig. 7 (right) shows the gain data. They were normally distributed in case of all but one source (i.e., 90\({}^{\circ}\) direction). However, significant outliers were found with respect to all sources, except from frontal direction (0\({}^{\circ}\)). Thus, the data was analyzed using a non-parametric Friedman test. The test indicated that the median normalized gain differed significantly between sources, \(X^{2}(2)=47.860,p<.001\). Pairwise comparisons using Dunn-Bonferroni tests indicated that median

Figure 7: Top: Radar charts showing the means and standard deviations pertaining to alpha, sharpness and normalized gain. Bottom: histograms showing the distribution of calibration values pertaining to alpha, sharpness and normalized gain. For the sake of visualization, gain values were normalized with an additive constant equals to 20 dB. The 0 db FS corresponds to this value in this chart, and to 60 db SPL.