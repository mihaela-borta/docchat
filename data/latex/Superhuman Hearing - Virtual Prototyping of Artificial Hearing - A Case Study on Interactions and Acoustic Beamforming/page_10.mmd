ANOVA found no significant difference in scores between conditions, \(F(2,32)=1.906,p=.165\). Finally, Fig. (b)b shows the results related to aggregate QUESI score obtained from the QUESI administered after exposure to realistic visuals. A non-parametric Friedman test was used for analysis. The test did not find a significant difference in scores between the three conditions, \(X^{2}(2)=1.529,p=.465\).

## 6 General discussion

In this study, we decided to let participants decide the shape of their virtual beamformer supported by the great flexibility of a calibration in VR. Answering to RQ1 (directly parameterization), they mainly adjusted the directional gain (see Fig. 7(bottom)), slightly changing the remaining parameters. No differentiations among interaction techniques were reported. Accordingly, one can assume that all individual parameters referred to the dominance of head-guided interaction metaphors. The typical calibration strategy primarily involved gain tuning for all available directions, followed by changes in alpha and sharpness on an individual basis. Such direction specific modulation resulted in a narrow polar lobe of the virtual beamformer, i.e., \(\alpha\approx 0.5\) meaning a cardioid shape in combination with \(\gamma>1\). For many participants, a SNR improvement ranging from 14 to 16 dB resulted from gain adjustments was already satisfactory in their virtual beamformer and in good agreement with the most elevated enhancement reported in the scientific literature [21]. Interestingly, there is also a general trend in parameter tuning for gain and for those participants who also consider \(\alpha\) and \(\gamma\) (see radar charts in Fig. 7): right directions exhibited less attenuation, less interference from adjacent directions (i.e., hyperspectral shape), and narrower beams. The average behavior of this virtual beamformer supported the lateralization of auditory attention and the right-ear advantage [30]. However, one can argue that directivity and sharpness parameters were not very useful for the given task, or users did not know how to take advantage of them. A future experimental session might consider an extensive training phase where users might be forced to manipulate one parameter at a time. Preferred and most useful parameter configurations could influence paring strategies.

Evaluating the same task with different levels of visual information has an important methodological validity for multisensory integration of auditory stimuli [31] which drives relevant research questions such as RQ2. In our study, the head-guided beamforming seemed to be highly influenced by the coordination of eye and head-movements, that facilitated human speech processing within a realistic audio-visual scene [68]. The statistical differentiation in pairing action time between abstract and realistic visuals for H and HE confirmed the improved timing performances in space orientation. On the other hand, HC did not follow the same integration mechanism because participants already had the focus on both speakers of a pair thanks to the virtual dual beamformer (i.e. head and hand was spatially anchored to the two identified speakers). Consequently, HC did not require advantages from visual anchors in the pairing action. Moreover, the statistically significant improvement in context number of pairs in HE (and a reduced variability compared to H and HC) with realistic visual details attested to the increase in reliability while coordinating head and eye.

RQ3 is closely related to RQ2 assuming that it was mainly related to the exploration period in the first pairing which is crucial for finding subsequent pairs and thus for the final performance. The root interaction H showed a positive trend for the first pairing time from abstract to realistic visual references, strengthened by the combination with a hand controller (HC) that exhibited a statistically significant differential unlike H alone. However, H and HC did not statistically differ in this metric suggesting future analyses with more participants and alternative performance metrics such as attention switching [53] and head movement quantity. On the other hand, the eye interaction resulted in a tendency of slower exploration. Gaze orientation towards a specific source required more time due to inefficiency in the eye-tracker and unoptimized transitions among focused directions that might require ad-hoc adjustments through gaze-contingent experiments [63]. This latter aspect will be subjected to future investigations.

Summarizing the outcomes regarding the virtual beamforming interactions, HC was the most efficient in the pairing task. The implementation of a dual beamformer could be considered a new artificial feature, perfectly matching task requirements with a superhuman hearing motivation. Moreover, task local decreased in the most challenging situation with minimal visual feedback (Fig. 9d). Since HC resulted in the more versatile technique, H suffered from the lack of visual details in terms of workload and pairing action time. HE closely derived from H, inheriting similar drawbacks at least in this speaker configuration. In particular, the effect of source displacement could be easily changed in future sessions following our methodology of virtual prototyping. High values on UX and QUESI questionnaires supported a future real development of such techniques. For the product design perspective in hearing aid industry, implementing eye tracking would require new technologies such as electroocologram (EOG) which measures eye movements based on skin mounted electrodes at a high frame-rate [23]. Similarly, supporting hands gesture control would require features comparable to mobile AR/MR devices (see [38] for recent trends in AR research).

Finally, user characterization in terms of listening abilities and pairing strategy will be analyzed within a larger pool of participants, supporting skills for the control of any new virtual beamformers.

## 7 Conclusion

This study provides a methodological and technological framework for virtual prototyping of the new generation of artificial and augmented hearing devices. Our main motivation is the identification of ideal sonic interactions in VR before investing resources in the actual development of real hardware/software technologies to be included in hearing aids and hearables. This case study disclosed potential superhuman abilities in the form of a virtual acoustic beamformer for the complex cocktail party problem. In particular, a task-specific beamforming control could support an effective experience in challenging (nearly impossible) listening situations. Interactions with a dual virtual beamformer resulted the best solution for a speaker pairing task in multi-source scenario.

The VR framework could be employed in different listening situations considering irregular displacement of listeners and different levels of visual information. Non-anechoic conditions will be considered to evaluate the impact of room acoustics which is a well-known problem for example classroom acoustics [6]. Moreover, dynamic rendering of VEs [48] and the addition of user walking [12] will be extremely relevant in evaluating superhuman hearing tools in a complex system, which will be more and more similar to reality.

It is worthwhile to notice that this virtual prototyping approach could easily integrate non-ideal or realistic virtual beamformers. Ambisonics encoding could incorporate acoustic measurements of directivity patterns from real hearing aids, substituting the pre-computation of spherical-harmonics based HRTFs in Eq. 7. Finally, the assumption of a priori knowledge of all virtual sound sources could be substituted by machine learning algorithms for auditory scene analysis [67]. With the simulated sound field acting as input, the virtual beamformer will be capable of detecting objects, classifying scenes and events.

## Acknowledgments

This study was supported by the internationalization grant of the 2016-2021 strategic program "Knowledge for the World" awarded by Aalborg University to Michele Geronazzo.

Figure 10: Boxplots visualizing results related to (a) the UEQ-S questionnaire and (b) the QUESI questionnaire in terms of medians, interquartile ranges, minimum and maximum values, and outliers.